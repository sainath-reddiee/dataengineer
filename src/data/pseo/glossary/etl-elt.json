[
  {
    "id": "dbt",
    "term": "dbt (Data Build Tool)",
    "slug": "dbt",
    "category": "etl-elt",
    "shortDefinition": "An open-source transformation tool that enables data analysts and engineers to transform data in their warehouse using SQL and software engineering best practices.",
    "fullDefinition": "dbt (data build tool) is an open-source command-line tool that enables data analysts and engineers to transform data in their data warehouse more effectively. It brings software engineering best practices like version control, testing, and documentation to analytics workflows.\n\n## What Makes dbt Different\n\nTraditional ETL tools require you to write transformations in proprietary languages or drag-and-drop interfaces. dbt takes a different approach:\n\n- **SQL-first**: Write transformations in pure SQL (or Python in dbt Core 1.3+)\n- **ELT, not ETL**: Assumes data is already loaded; focuses only on the T (transform)\n- **Modular**: Build reusable models that reference each other\n- **Version Controlled**: Store all code in Git for collaboration\n- **Tested**: Define tests to validate data quality\n- **Documented**: Auto-generate documentation from your code\n\n## Core Concepts\n\n1. **Models**: SQL files that define transformations. Each model compiles to a SELECT statement and creates a table or view.\n\n2. **Sources**: Declare raw tables loaded by your ingestion tools (Fivetran, Airbyte, etc.)\n\n3. **Tests**: Assertions about your data (unique, not null, relationships)\n\n4. **Documentation**: Describe models and columns; dbt generates a searchable doc site\n\n5. **Macros**: Reusable SQL snippets (like functions) using Jinja templating\n\n## dbt Core vs dbt Cloud\n\n- **dbt Core**: Free, open-source CLI tool you run locally or in CI/CD\n- **dbt Cloud**: Managed platform with IDE, scheduling, and collaboration features\n\n## Common dbt Commands\n\n```bash\ndbt run          # Execute all models\ndbt test         # Run all tests\ndbt docs generate  # Generate documentation\ndbt build        # Run + test in dependency order\n```\n\n## Why Teams Adopt dbt\n\n- **Speed**: Analysts can own transformations without waiting for engineers\n- **Quality**: Tests catch data issues before they reach dashboards\n- **Collaboration**: Git-based workflow enables code review and teamwork\n- **Maintainability**: Modular, documented code is easier to understand and update",
    "keyPoints": [
      "SQL-first transformation tool for data warehouses",
      "Follows ELT pattern (transform after loading)",
      "Built-in testing framework for data quality",
      "Auto-generated documentation from code",
      "Git-based workflow for version control"
    ],
    "faqs": [
      {
        "question": "What is dbt used for?",
        "answer": "dbt is used for transforming raw data in a data warehouse into analytics-ready datasets. It allows data teams to write modular SQL, test data quality, and generate documentation—all using software engineering best practices."
      },
      {
        "question": "Is dbt ETL or ELT?",
        "answer": "dbt is an ELT tool. It focuses only on the T (Transform) step, assuming data has already been Extracted and Loaded into your warehouse by other tools like Fivetran or Airbyte."
      },
      {
        "question": "Is dbt free to use?",
        "answer": "dbt Core is 100% free and open-source. dbt Cloud offers a free tier for individuals, with paid plans for teams that need scheduling, IDE, and collaboration features."
      },
      {
        "question": "What databases does dbt support?",
        "answer": "dbt supports all major cloud data warehouses including Snowflake, BigQuery, Redshift, Databricks, and many others through community-maintained adapters."
      }
    ],
    "relatedTerms": [
      "snowflake",
      "data-warehouse",
      "etl",
      "data-modeling",
      "sql"
    ],
    "relatedTools": [
      "Snowflake",
      "BigQuery",
      "Redshift",
      "Fivetran",
      "Airbyte"
    ],
    "externalLinks": [
      {
        "title": "dbt Documentation",
        "url": "https://docs.getdbt.com/"
      },
      {
        "title": "dbt Learn (Free Courses)",
        "url": "https://courses.getdbt.com/"
      }
    ],
    "keywords": [
      "dbt",
      "data build tool",
      "dbt tutorial",
      "dbt vs etl",
      "dbt testing"
    ],
    "lastUpdated": "2026-01-21"
  },
  {
    "id": "etl",
    "term": "ETL (Extract, Transform, Load)",
    "slug": "etl",
    "category": "etl-elt",
    "shortDefinition": "A data integration process that extracts data from source systems, transforms it into a suitable format, and loads it into a target data warehouse or database.",
    "fullDefinition": "ETL stands for Extract, Transform, Load—the traditional process for moving and preparing data for analytics. It's the backbone of data warehousing and has been used for decades to integrate data from multiple sources.\n\n## The Three Stages\n\n### 1. Extract\nPull data from various source systems:\n- Databases (MySQL, PostgreSQL, Oracle)\n- SaaS applications (Salesforce, HubSpot)\n- APIs (REST, GraphQL)\n- Files (CSV, JSON, XML)\n- Streaming sources (Kafka, webhooks)\n\n### 2. Transform\nApply business logic to prepare data:\n- **Cleaning**: Remove duplicates, handle nulls\n- **Standardization**: Unify formats (dates, currencies)\n- **Aggregation**: Summarize data for reporting\n- **Enrichment**: Add calculated fields or lookups\n- **Validation**: Apply business rules\n\n### 3. Load\nWrite transformed data to the target:\n- Data warehouses (Snowflake, BigQuery)\n- Data lakes (S3, Azure Data Lake)\n- Operational databases\n\n## ETL vs ELT\n\nThe rise of cloud data warehouses has popularized ELT:\n\n| Aspect | ETL | ELT |\n|--------|-----|-----|\n| Transform Location | Before loading (staging server) | After loading (in warehouse) |\n| Best For | Limited warehouse compute | Powerful cloud warehouses |\n| Flexibility | Less flexible (predefined) | More flexible (transform anytime) |\n| Tools | Informatica, SSIS, Talend | dbt, Snowflake, BigQuery |\n\nModern data stacks often use **ELT**: load raw data first, then transform using tools like dbt.\n\n## Popular ETL/ELT Tools\n\n- **Fivetran**: Automated, fully managed connectors\n- **Airbyte**: Open-source data integration platform\n- **Stitch**: Simple, developer-friendly pipelines\n- **dbt**: Transformation layer (the T in ELT)\n- **Apache Airflow**: Workflow orchestration for custom ETL\n\n## Best Practices\n\n1. **Incremental Loading**: Only process new/changed data\n2. **Idempotency**: Running the same job twice should produce the same result\n3. **Monitoring**: Track data quality and pipeline health\n4. **Documentation**: Maintain data lineage and transformation logic",
    "keyPoints": [
      "Three-step process: Extract, Transform, Load",
      "Foundational process for data warehousing",
      "Modern alternative is ELT (transform in warehouse)",
      "Key tools: Fivetran, Airbyte, dbt",
      "Enables single source of truth for analytics"
    ],
    "faqs": [
      {
        "question": "What is ETL in simple terms?",
        "answer": "ETL is a process that pulls data from various sources (Extract), cleans and transforms it into a usable format (Transform), and stores it in a data warehouse for analysis (Load)."
      },
      {
        "question": "What is the difference between ETL and ELT?",
        "answer": "In ETL, data is transformed before loading into the target. In ELT, raw data is loaded first, then transformed inside the data warehouse. ELT is preferred for cloud warehouses with powerful compute."
      },
      {
        "question": "What tools are used for ETL?",
        "answer": "Popular ETL tools include Fivetran, Airbyte, Stitch, Talend, Informatica, and Apache NiFi. For the transformation layer, dbt is widely used in modern data stacks."
      },
      {
        "question": "Why is ETL important?",
        "answer": "ETL ensures that data from different sources is cleaned, standardized, and integrated into a single repository. This enables accurate reporting, analytics, and data-driven decision making."
      }
    ],
    "relatedTerms": [
      "data-warehouse",
      "dbt",
      "data-pipeline",
      "data-integration"
    ],
    "relatedTools": [
      "Fivetran",
      "Airbyte",
      "dbt",
      "Stitch",
      "Talend"
    ],
    "externalLinks": [
      {
        "title": "ETL Explained - IBM",
        "url": "https://www.ibm.com/topics/etl"
      },
      {
        "title": "ETL vs ELT - Fivetran",
        "url": "https://www.fivetran.com/blog/etl-vs-elt"
      }
    ],
    "keywords": [
      "etl",
      "extract transform load",
      "etl vs elt",
      "etl tools",
      "etl process"
    ],
    "lastUpdated": "2026-01-21"
  }
]