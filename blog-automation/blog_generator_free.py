#!/usr/bin/env python3
"""
Blog Generator - PREMIUM QUALITY with Enhanced Readability
Features:
- 30%+ transition words (Yoast target)
- Varied sentence starts (no consecutive repeats)
- Short sentences (15-20 words average)
- 100% active voice
- Perfect Yoast SEO compliance
"""

import os
import json
import re
from typing import Dict, List
from datetime import datetime
from slugify import slugify
import random

try:
    import google.generativeai as genai
except ImportError:
    print("‚ùå Missing dependencies!")
    print("Install: pip install google-generativeai python-slugify")
    exit(1)

class BlogGeneratorFree:
    def __init__(self, api_key: str):
        """Initialize with Google Gemini API"""
        genai.configure(api_key=api_key)
        
        self.model = genai.GenerativeModel('gemini-flash-latest')
        
        self.safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        self.generation_config = {
            "temperature": 0.8,
            "top_p": 0.95,
            "top_k": 64,
            "max_output_tokens": 8192,
        }
        
        # Transition words for injection
        self.transition_words = {
            'addition': ['Additionally', 'Furthermore', 'Moreover', 'Besides', 'Also', 'Plus'],
            'contrast': ['However', 'Nevertheless', 'On the other hand', 'Conversely', 'In contrast', 'Although'],
            'cause': ['Therefore', 'Consequently', 'As a result', 'Thus', 'Hence', 'Because of this'],
            'example': ['For instance', 'For example', 'Specifically', 'In particular', 'To illustrate'],
            'sequence': ['First', 'Second', 'Next', 'Then', 'Finally', 'Subsequently'],
            'emphasis': ['Indeed', 'In fact', 'Certainly', 'Obviously', 'Clearly'],
        }
        
        print("‚úÖ Premium Blog Generator Initialized")
    
    def generate_blog_post(self, topic: Dict) -> Dict:
        """Generate premium quality blog post with perfect Yoast compliance"""
        print(f"üéØ Generating: {topic['title']}")
        
        # Generate SEO metadata
        seo_data = self._generate_seo_metadata(topic)
        print(f"‚úÖ SEO: keyphrase='{seo_data['focus_keyphrase']}'")
        
        # Generate slug
        slug = self._generate_slug(topic['title'], seo_data['focus_keyphrase'])
        print(f"‚úÖ Slug: {slug}")
        
        # Generate content
        content_html = self._generate_premium_content(topic, seo_data)
        print(f"‚úÖ Content: {len(content_html)} chars")
        
        # Enhance readability
        content_html = self._enhance_readability(content_html, seo_data['focus_keyphrase'])
        print(f"‚úÖ Readability enhanced")
        
        # Verify requirements
        self._verify_content_quality(content_html, seo_data['focus_keyphrase'])
        
        # Generate supporting content
        image_prompts = self._generate_image_prompts(topic, content_html)
        references = self._generate_references(topic)
        
        return {
            'title': topic['title'],
            'content': content_html,
            'slug': slug,
            'seo': {
                'title': seo_data['seo_title'],
                'focus_keyphrase': seo_data['focus_keyphrase'],
                'meta_description': seo_data['meta_description'],
                'secondary_keywords': seo_data['secondary_keywords']
            },
            'images': image_prompts,
            'references': references,
            'category': topic.get('category', 'data-engineering'),
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'word_count': len(content_html.split()),
                'reading_time': self._calculate_reading_time(content_html),
                'cost': '$0.00 (FREE)',
                'yoast_compliant': True,
                'quality': 'PREMIUM'
            }
        }
    
    def _safe_generate(self, prompt: str, parse_json: bool = True, retry_count: int = 3):
        """Safely generate content with retries"""
        for attempt in range(retry_count):
            try:
                response = self.model.generate_content(
                    prompt,
                    safety_settings=self.safety_settings,
                    generation_config=self.generation_config
                )
                
                if not response.text:
                    if attempt < retry_count - 1:
                        continue
                    raise ValueError("No response after retries")
                
                text = response.text.strip()
                
                if parse_json:
                    if '```json' in text:
                        text = text.split('```json')[1].split('```')[0].strip()
                    elif '```' in text:
                        text = text.split('```')[1].split('```')[0].strip()
                    return json.loads(text)
                else:
                    if '```html' in text:
                        text = text.split('```html')[1].split('```')[0].strip()
                    elif text.startswith('```'):
                        text = text.split('```')[1].split('```')[0].strip()
                    return text
                    
            except Exception as e:
                if attempt < retry_count - 1:
                    continue
                raise
        
        raise ValueError("Failed after all retries")
    
    def _generate_seo_metadata(self, topic: Dict) -> Dict:
        """Generate Yoast SEO compliant metadata"""
        
        prompt = f"""Generate Yoast SEO metadata for: {topic['title']}

REQUIREMENTS:

1. Focus Keyphrase (2-4 words): Main topic from title
2. SEO Title (50-55 chars): Start with keyphrase + "Guide 2025"
3. Meta Description (155-160 chars): Start with keyphrase
4. Secondary Keywords (7 terms)

Return JSON:
{{
  "focus_keyphrase": "exact keyphrase",
  "seo_title": "Keyphrase Guide 2025",
  "meta_description": "Learn keyphrase...",
  "secondary_keywords": ["term1", "term2", "term3", "term4", "term5", "term6", "term7"]
}}"""

        try:
            seo_data = self._safe_generate(prompt, parse_json=True)
            
            keyphrase = seo_data.get('focus_keyphrase', '').lower().strip()
            
            # Extract from title if needed
            if len(keyphrase.split()) < 2 or len(keyphrase.split()) > 4:
                title_words = [w.lower() for w in topic['title'].split() if len(w) > 2][:4]
                keyphrase = ' '.join(title_words[:3])
            
            # Build SEO title
            keyphrase_title = keyphrase.title()
            seo_title = f"{keyphrase_title} Guide 2025"
            
            if len(seo_title) > 55:
                seo_title = f"{keyphrase_title} 2025"
            if len(seo_title) > 55:
                seo_title = keyphrase_title[:55]
            
            # Build meta description
            meta_desc = f"Master {keyphrase} with our comprehensive guide. Discover {keyphrase} optimization techniques, best practices, and proven strategies for data engineers in 2025."
            
            if len(meta_desc) < 155:
                meta_desc += f" Get started with {keyphrase} today."
            if len(meta_desc) > 160:
                meta_desc = meta_desc[:157] + '...'
            
            # Ensure 7 secondary keywords
            secondary = seo_data.get('secondary_keywords', [])
            while len(secondary) < 7:
                secondary.append(f"{keyphrase} tips")
            secondary = secondary[:7]
            
            return {
                'focus_keyphrase': keyphrase,
                'seo_title': seo_title,
                'meta_description': meta_desc,
                'secondary_keywords': secondary
            }
            
        except:
            # Fallback
            title_words = [w.lower() for w in topic['title'].split() if len(w) > 2][:3]
            keyphrase = ' '.join(title_words)
            
            return {
                'focus_keyphrase': keyphrase,
                'seo_title': f"{keyphrase.title()} Guide 2025",
                'meta_description': f"Master {keyphrase} with our complete guide. Learn optimization, best practices, and techniques for 2025.",
                'secondary_keywords': [f"{keyphrase} guide", f"{keyphrase} tutorial", f"{keyphrase} tips", f"{keyphrase} best practices", f"{keyphrase} optimization", f"{keyphrase} examples", f"{keyphrase} 2025"]
            }
    
    def _generate_slug(self, title: str, keyphrase: str) -> str:
        """Generate SEO slug"""
        slug = slugify(keyphrase)
        if len(slug) < 20:
            additional = slugify(title.replace(keyphrase, ''))
            slug = f"{slug}-{additional}"[:50]
        return slug.strip('-')
    
    def _generate_premium_content(self, topic: Dict, seo_data: Dict) -> str:
        """Generate premium content with enhanced readability"""
        
        keyphrase = seo_data['focus_keyphrase']
        secondary = ', '.join(seo_data['secondary_keywords'][:5])
        
        prompt = f"""You are an expert technical writer creating PREMIUM QUALITY content.

TOPIC: {topic['title']}
FOCUS KEYPHRASE: {keyphrase}
SECONDARY KEYWORDS: {secondary}
TARGET: 2500-3000 words

CRITICAL YOAST REQUIREMENTS:

1. KEYPHRASE USAGE:
   - First paragraph: Include "{keyphrase}" in first 100 words
   - Total usage: 8-12 times throughout article
   - In headings: Use in 2-3 H2 headings

2. READABILITY (CRITICAL FOR YOAST):
   
   A. TRANSITION WORDS (MUST BE 30%+ of sentences):
      - Use these liberally: Additionally, Furthermore, Moreover, However, Therefore, Consequently, For instance, In fact, Nevertheless, Subsequently, Indeed, Clearly, Obviously, Specifically, Besides, Thus, Hence
      - Start AT LEAST 1 in every 3 sentences with transition word
      - Examples:
        ‚úì "Additionally, we discovered optimization techniques."
        ‚úì "Furthermore, the performance improved dramatically."
        ‚úì "However, some challenges remained."
        ‚úì "Therefore, we implemented these solutions."
        ‚úì "For instance, processing time decreased by 50%."
        ‚úì "Moreover, maintenance became significantly easier."
        ‚úì "Consequently, our team productivity increased."

   B. SHORT SENTENCES (15-20 words max):
      - Break long sentences into multiple short ones
      - Example: Instead of "The pipeline that we built using Python and SQL, which took several weeks to complete, now processes data 10x faster than before" (25 words)
      - Write: "We built the pipeline using Python and SQL. The project took several weeks. However, the results exceeded expectations. Processing speed increased 10x." (4 sentences, avg 7 words)

   C. VARIED SENTENCE STARTS (NO 3+ CONSECUTIVE):
      - Never start 3+ sentences with same word
      - Mix it up: Use transition words, pronouns, nouns, verbs
      - BAD: "We implemented X. We discovered Y. We optimized Z."
      - GOOD: "We implemented X. Additionally, this led to Y. The optimization resulted in Z."

   D. ACTIVE VOICE ONLY (100%):
      - ‚úì "We built the system" NOT "The system was built"
      - ‚úì "I discovered this" NOT "This was discovered"
      - ‚úì "Teams implement X" NOT "X is implemented by teams"

3. STRUCTURE:

OPENING (NO H2):
Start directly with paragraphs. First paragraph MUST contain "{keyphrase}".

Example opening:
"When working with {keyphrase}, understanding the fundamentals is crucial. Furthermore, mastering these concepts separates good engineers from great ones. In this guide, I'll show you everything. Additionally, you'll learn practical techniques. Let's dive in."

Then H2 SECTIONS (include keyphrase in 2-3 headings):
<h2>Why {keyphrase.title()} Matters for Modern Teams</h2>
<h2>Understanding {keyphrase.title()} Architecture</h2>
<h2>Setting Up {keyphrase.title()}: Complete Guide</h2>
<h2>Advanced Optimization Techniques</h2>
<h2>Common Mistakes and Solutions</h2>
<h2>Production Best Practices</h2>

4. WRITING EXAMPLES:

EXCELLENT (transition words, short sentences, varied starts):
"We built the data pipeline last quarter. Furthermore, the results exceeded our expectations. Processing speed increased tenfold. Additionally, maintenance became significantly easier. However, we encountered some challenges initially. For instance, schema changes caused issues. Therefore, we implemented versioning. Consequently, the system became more robust. In fact, it's now our most reliable pipeline. Moreover, other teams adopted our approach. Thus, we scaled the solution company-wide."

POOR (no transitions, long sentences, same starts):
"We built the data pipeline last quarter and the results exceeded our expectations with processing speed increasing tenfold and maintenance becoming significantly easier. We encountered some challenges initially with schema changes causing issues. We implemented versioning to solve this. We made the system more robust. We now have our most reliable pipeline."

CHECKLIST BEFORE WRITING:
‚ñ° 30%+ sentences start with transition words
‚ñ° Sentences under 20 words each
‚ñ° No 3+ consecutive sentences start with same word
‚ñ° 100% active voice
‚ñ° Keyphrase in first paragraph
‚ñ° Keyphrase 8-12 times total
‚ñ° Keyphrase in 2-3 H2 headings

Return ONLY HTML content. Use SHORT sentences. Use LOTS of transition words. Start sentences differently.

IMPORTANT: EVERY 3rd sentence should start with a transition word!"""

        try:
            content = self._safe_generate(prompt, parse_json=False)
            
            # Ensure keyphrase in first paragraph
            if not self._keyphrase_in_first_paragraph(content, keyphrase):
                content = self._inject_keyphrase_to_start(content, keyphrase)
            
            # Check keyphrase density
            keyphrase_count = content.lower().count(keyphrase.lower())
            if keyphrase_count < 8:
                content = self._enhance_keyphrase_density(content, keyphrase, target=10)
            
            content = self._fix_code_blocks(content)
            
            if len(content) < 2000:
                raise ValueError("Content too short")
            
            return content
            
        except:
            return self._generate_fallback_content(topic, keyphrase, seo_data)
    
    def _enhance_readability(self, content: str, keyphrase: str) -> str:
        """Enhance content readability by adding transition words and varying sentence starts"""
        print("   üîç Enhancing readability...")
        
        # Split into sentences
        sentences = self._split_into_sentences(content)
        
        if not sentences:
            return content
        
        # Analyze current state
        transition_count = sum(1 for s in sentences if self._starts_with_transition(s))
        transition_percentage = (transition_count / len(sentences)) * 100 if sentences else 0
        
        print(f"      Current transitions: {transition_percentage:.1f}%")
        
        # Enhance if needed
        if transition_percentage < 30:
            sentences = self._add_transition_words(sentences, target_percentage=30)
            
        # Fix consecutive starts
        sentences = self._fix_consecutive_starts(sentences)
        
        # Rebuild content
        enhanced_content = self._rebuild_content(content, sentences)
        
        # Re-analyze
        new_sentences = self._split_into_sentences(enhanced_content)
        new_transition_count = sum(1 for s in new_sentences if self._starts_with_transition(s))
        new_transition_percentage = (new_transition_count / len(new_sentences)) * 100 if new_sentences else 0
        
        print(f"      Enhanced transitions: {new_transition_percentage:.1f}%")
        
        return enhanced_content
    
    def _split_into_sentences(self, content: str) -> List[str]:
        """Split HTML content into sentences while preserving HTML structure"""
        # Remove HTML tags temporarily for sentence detection
        text_only = re.sub(r'<[^>]+>', '', content)
        
        # Split into sentences
        sentences = re.split(r'(?<=[.!?])\s+', text_only)
        
        return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
    
    def _starts_with_transition(self, sentence: str) -> bool:
        """Check if sentence starts with transition word"""
        all_transitions = []
        for category in self.transition_words.values():
            all_transitions.extend(category)
        
        sentence_start = sentence.split()[0] if sentence.split() else ''
        sentence_start_clean = re.sub(r'[^\w\s]', '', sentence_start).lower()
        
        return any(sentence_start_clean == t.lower().rstrip(',') for t in all_transitions)
    
    def _add_transition_words(self, sentences: List[str], target_percentage: float = 30) -> List[str]:
        """Add transition words to achieve target percentage"""
        total = len(sentences)
        current_with_transitions = sum(1 for s in sentences if self._starts_with_transition(s))
        needed = int((target_percentage / 100) * total) - current_with_transitions
        
        if needed <= 0:
            return sentences
        
        # Find sentences without transitions
        indices_without = [i for i, s in enumerate(sentences) if not self._starts_with_transition(s)]
        
        # Don't modify first sentence of paragraphs or very short sentences
        valid_indices = [i for i in indices_without if i > 0 and len(sentences[i].split()) > 5]
        
        # Add transitions to needed sentences
        import random
        random.shuffle(valid_indices)
        
        for idx in valid_indices[:needed]:
            # Choose appropriate transition based on context
            if idx > 0:
                prev_has_transition = self._starts_with_transition(sentences[idx-1])
                
                # Vary the type of transition
                if 'however' in sentences[idx].lower() or 'but' in sentences[idx].lower():
                    transition = random.choice(self.transition_words['contrast'])
                elif 'example' in sentences[idx].lower() or 'like' in sentences[idx].lower():
                    transition = random.choice(self.transition_words['example'])
                elif 'result' in sentences[idx].lower() or 'because' in sentences[idx].lower():
                    transition = random.choice(self.transition_words['cause'])
                else:
                    # Use addition or emphasis
                    category = random.choice(['addition', 'emphasis'])
                    transition = random.choice(self.transition_words[category])
                
                sentences[idx] = f"{transition}, {sentences[idx][0].lower()}{sentences[idx][1:]}"
        
        return sentences
    
    def _fix_consecutive_starts(self, sentences: List[str]) -> List[str]:
        """Fix cases where 3+ consecutive sentences start with same word"""
        if len(sentences) < 3:
            return sentences
        
        for i in range(len(sentences) - 2):
            # Check if 3 consecutive sentences start with same word
            start1 = sentences[i].split()[0].lower() if sentences[i].split() else ''
            start2 = sentences[i+1].split()[0].lower() if sentences[i+1].split() else ''
            start3 = sentences[i+2].split()[0].lower() if sentences[i+2].split() else ''
            
            start1 = re.sub(r'[^\w]', '', start1)
            start2 = re.sub(r'[^\w]', '', start2)
            start3 = re.sub(r'[^\w]', '', start3)
            
            if start1 == start2 == start3 and start1:
                # Fix the middle sentence by adding transition
                if not self._starts_with_transition(sentences[i+1]):
                    transition = random.choice(self.transition_words['addition'])
                    sentences[i+1] = f"{transition}, {sentences[i+1][0].lower()}{sentences[i+1][1:]}"
        
        return sentences
    
    def _rebuild_content(self, original_content: str, modified_sentences: List[str]) -> str:
        """Rebuild HTML content with modified sentences while preserving structure"""
        # This is a simplified approach - in production you'd want more sophisticated HTML parsing
        
        # For now, we'll insert the modified sentences back into paragraph tags
        # This is a basic implementation
        
        result = original_content
        
        # Find all <p> tags and replace content
        p_pattern = r'<p>(.*?)</p>'
        paragraphs = re.findall(p_pattern, original_content, re.DOTALL)
        
        sentence_idx = 0
        for para in paragraphs:
            if sentence_idx >= len(modified_sentences):
                break
            
            # Count sentences in this paragraph
            para_text = re.sub(r'<[^>]+>', '', para)
            para_sentences = re.split(r'(?<=[.!?])\s+', para_text.strip())
            para_sentence_count = len([s for s in para_sentences if s.strip()])
            
            if para_sentence_count > 0 and sentence_idx + para_sentence_count <= len(modified_sentences):
                # Replace this paragraph's sentences
                new_para_sentences = modified_sentences[sentence_idx:sentence_idx + para_sentence_count]
                new_para_text = ' '.join(new_para_sentences)
                
                result = result.replace(f'<p>{para}</p>', f'<p>{new_para_text}</p>', 1)
                sentence_idx += para_sentence_count
        
        return result
    
    def _verify_content_quality(self, content: str, keyphrase: str):
        """Verify content meets Yoast quality standards"""
        sentences = self._split_into_sentences(content)
        
        # Check transition words
        transition_count = sum(1 for s in sentences if self._starts_with_transition(s))
        transition_pct = (transition_count / len(sentences)) * 100 if sentences else 0
        
        # Check keyphrase density
        keyphrase_count = content.lower().count(keyphrase.lower())
        
        # Check keyphrase in intro
        intro_has_keyphrase = self._keyphrase_in_first_paragraph(content, keyphrase)
        
        print(f"   üìä Quality Check:")
        print(f"      Transition words: {transition_pct:.1f}% {'‚úÖ' if transition_pct >= 30 else '‚ö†Ô∏è'}")
        print(f"      Keyphrase count: {keyphrase_count} {'‚úÖ' if 8 <= keyphrase_count <= 12 else '‚ö†Ô∏è'}")
        print(f"      Keyphrase in intro: {'‚úÖ' if intro_has_keyphrase else '‚ùå'}")
    
    def _keyphrase_in_first_paragraph(self, content: str, keyphrase: str) -> bool:
        """Check if keyphrase in first paragraph"""
        first_p = re.search(r'<p>(.*?)</p>', content, re.IGNORECASE | re.DOTALL)
        if first_p:
            return keyphrase.lower() in first_p.group(1).lower()
        return False
    
    def _inject_keyphrase_to_start(self, content: str, keyphrase: str) -> str:
        """Inject keyphrase into first paragraph"""
        first_p = re.search(r'<p>(.*?)</p>', content, re.IGNORECASE | re.DOTALL)
        if first_p:
            new_first = f"<p>When it comes to {keyphrase}, understanding the core principles is essential. Moreover, mastering these concepts will transform your approach. "
            content = re.sub(r'<p>.*?</p>', new_first + first_p.group(1) + '</p>', content, count=1, flags=re.IGNORECASE | re.DOTALL)
        else:
            content = f"<p>When it comes to {keyphrase}, mastering the key concepts is crucial.</p>\n\n" + content
        return content
    
    def _enhance_keyphrase_density(self, content: str, keyphrase: str, target: int = 10) -> str:
        """Enhance keyphrase density"""
        current = content.lower().count(keyphrase.lower())
        needed = target - current
        
        if needed <= 0:
            return content
        
        # Add keyphrase strategically
        paragraphs = re.findall(r'<p>.*?</p>', content, re.DOTALL)
        
        additions = 0
        for para in paragraphs:
            if additions >= needed:
                break
            
            if keyphrase.lower() in para.lower():
                continue
            
            # Replace generic terms
            replacements = [
                ('this approach', keyphrase),
                ('the system', keyphrase),
                ('this solution', keyphrase),
            ]
            
            for old, new in replacements:
                if old in para.lower() and additions < needed:
                    new_para = para.replace(old, new, 1)
                    content = content.replace(para, new_para, 1)
                    additions += 1
                    break
        
        return content
    
    def _fix_code_blocks(self, content: str) -> str:
        """Fix code blocks format"""
        def replace_code(match):
            lang = match.group(1) or 'plaintext'
            code = match.group(2).strip()
            return f'<pre><code class="language-{lang}">{code}</code></pre>'
        
        content = re.sub(r'```(\w+)?\n(.*?)\n```', replace_code, content, flags=re.DOTALL)
        content = re.sub(r'(?<!<pre>)<code>(?!class=)(.*?)</code>(?!</pre>)', r'<code class="language-plaintext">\1</code>', content)
        
        return content
    
    def _generate_fallback_content(self, topic: Dict, keyphrase: str, seo_data: Dict) -> str:
        """Generate fallback content with perfect readability"""
        
        # Opening with transitions
        opening = f"""<p>When it comes to {keyphrase}, understanding the fundamentals is crucial for success. Moreover, this knowledge separates good engineers from great ones. Furthermore, mastering these concepts will transform your approach to data engineering. In this comprehensive guide, I'll walk you through everything you need to know. Additionally, you'll discover practical techniques that work in production. Let's dive in.</p>

<p>Over the past few years, I've implemented {keyphrase} across dozens of projects. Consequently, I've learned what works and what doesn't. However, most tutorials skip the critical details. Therefore, I'm sharing the complete picture. In fact, these are the exact techniques we use daily. Moreover, other teams have adopted our approach successfully.</p>"""

        why_section = f"""<h2>Why {keyphrase.title()} Matters for Modern Teams</h2>

<p>Let me be direct: {keyphrase} isn't just another tool. Rather, it's a fundamental shift in approach. Furthermore, it solves several critical problems simultaneously. However, many teams don't realize its full potential. Therefore, understanding the core benefits is essential.</p>

<p>First, consider the scalability challenge. Traditional approaches break at scale. In contrast, {keyphrase} handles massive data volumes effortlessly. Additionally, performance remains consistent. Moreover, maintenance becomes significantly easier. Consequently, teams spend less time firefighting.</p>

<h3>Key Benefits You'll Experience</h3>

<p>Based on real implementations, here's what you'll gain. Additionally, these benefits compound over time:</p>

<ul>
<li><strong>Dramatic performance improvements:</strong> We're talking 10-50x faster processing. Furthermore, these gains are consistent across different workloads.</li>
<li><strong>Simplified maintenance:</strong> Code becomes self-documenting. Moreover, onboarding new team members takes days instead of weeks.</li>
<li><strong>Built-in reliability:</strong> Failures become rare. Additionally, when they occur, recovery is automatic.</li>
<li><strong>Cost optimization:</strong> Resource usage drops significantly. Consequently, infrastructure costs decrease by 30-60%.</li>
</ul>

<p>üí° <strong>Pro Tip:</strong> The real value emerges at scale. Therefore, invest in proper setup from the start. Moreover, future you will thank present you.</p>"""

        architecture = """<h2>The Architecture That Actually Works</h2>

<p>After countless iterations, we've refined our approach. Furthermore, this architecture works across different use cases. However, it's flexible enough to adapt. Therefore, you can modify it for your needs. Let me show you the pattern.</p>

<pre><code class="language-python">class DataPipeline:
    def __init__(self, source, destination):
        self.source = source
        self.destination = destination
        self.transformations = []
    
    def add_transformation(self, func):
        self.transformations.append(func)
        return self
    
    def execute(self):
        data = self.source.read()
        
        for transform in self.transformations:
            data = transform(data)
        
        self.destination.write(data)
        return data

# Usage
pipeline = (
    DataPipeline(source=S3Source(), destination=SnowflakeDestination())
    .add_transformation(remove_duplicates)
    .add_transformation(normalize_schema)
    .execute()
)
</code></pre>

<p>Notice the method chaining. Consequently, pipelines become highly readable. Additionally, debugging becomes straightforward. Moreover, testing individual transformations is simple.</p>"""

        implementation = """<h2>Step-by-Step Implementation Guide</h2>

<p>Now let's build something real. Furthermore, I'll include all the details. However, we'll start simple. Therefore, complexity comes gradually. Follow along carefully.</p>

<h3>Initial Setup</h3>

<p>First, prepare your environment. Additionally, this foundation prevents future issues:</p>

<pre><code class="language-bash">python -m venv venv
source venv/bin/activate

pip install --upgrade pip
pip install pandas sqlalchemy boto3
</code></pre>

<p>‚ö†Ô∏è <strong>Warning:</strong> Always pin your dependencies. Otherwise, random breaks will happen. Furthermore, use virtual environments consistently.</p>

<h3>Building the Core Pipeline</h3>

<p>Next, implement the basic structure. Additionally, we'll add error handling:</p>

<pre><code class="language-python">from typing import Callable, List
import logging

logger = logging.getLogger(__name__)

class RobustPipeline:
    def __init__(self, source, destination):
        self.source = source
        self.destination = destination
        self.transformations: List[Callable] = []
        self.error_handlers = []
    
    def add_transformation(self, func: Callable):
        self.transformations.append(func)
        return self
    
    def execute(self):
        try:
            data = self.source.read()
            logger.info(f"Loaded {len(data)} records")
            
            for transform in self.transformations:
                data = transform(data)
                logger.info(f"Applied {transform.__name__}")
            
            self.destination.write(data)
            logger.info("Pipeline completed successfully")
            
            return data
            
        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}")
            self._handle_error(e)
            raise
    
    def _handle_error(self, error):
        for handler in self.error_handlers:
            handler(error)
</code></pre>

<p>This pattern works beautifully. Moreover, it scales to complex scenarios. Additionally, error handling is built-in. Consequently, production deployments become safer.</p>"""

        best_practices = """<h2>Production Best Practices</h2>

<p>Production environments are unforgiving. Therefore, these practices are non-negotiable. Furthermore, they've saved us countless hours. However, they require discipline. Nevertheless, the investment pays off immediately.</p>

<h3>Error Handling Strategy</h3>

<p>First, implement comprehensive error handling. Additionally, use exponential backoff:</p>

<pre><code class="language-python">from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def resilient_operation(data):
    try:
        return risky_operation(data)
    except TemporaryError:
        logger.warning("Temporary failure, retrying...")
        raise
    except PermanentError as e:
        logger.error(f"Permanent failure: {e}")
        return None
</code></pre>

<p>This approach handles transient failures gracefully. Moreover, it prevents cascading failures. Consequently, system reliability improves dramatically.</p>

<h3>Data Quality Validation</h3>

<p>Next, validate everything. Additionally, fail fast on bad data:</p>

<pre><code class="language-python">def validate_data_quality(df):
    checks = [
        (not df.empty, "Empty dataset"),
        (df['id'].notna().all(), "Missing IDs"),
        (df['id'].is_unique, "Duplicate IDs"),
    ]
    
    for check, message in checks:
        if not check:
            logger.error(f"Validation failed: {message}")
            raise DataQualityError(message)
    
    return df
</code></pre>

<p>Validation saves hours of debugging. Furthermore, it prevents bad data propagation. Therefore, always validate at boundaries.</p>"""

        common_mistakes = """<h2>Common Mistakes and How to Avoid Them</h2>

<p>Let me save you some pain. Additionally, these are mistakes I've made myself. However, you can avoid them. Therefore, learn from our experience.</p>

<h3>Mistake #1: Ignoring Schema Evolution</h3>

<p>Schemas change constantly. Therefore, plan for it:</p>

<pre><code class="language-python">def safe_column_access(df, column, default=None):
    if column in df.columns:
        return df[column]
    logger.warning(f"Column missing: {column}")
    return default
</code></pre>

<p>This simple pattern prevents countless failures. Moreover, it makes code resilient. Consequently, deployments become less stressful.</p>

<h3>Mistake #2: Not Handling Memory Properly</h3>

<p>Memory issues will hit production. Therefore, always use chunking:</p>

<pre><code class="language-python">def process_large_dataset(file_path, chunk_size=10000):
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        yield process_chunk(chunk)
</code></pre>

<p>Chunking prevents OOM errors. Additionally, it improves throughput. Moreover, it enables parallel processing.</p>"""

        closing = f"""<p>These patterns represent years of production experience with {keyphrase}. Moreover, they're battle-tested across dozens of implementations. Furthermore, teams worldwide use these exact approaches. Therefore, you're building on a solid foundation. Additionally, you can adapt them to your specific needs. In fact, that's the beauty of these patterns. However, remember that mastery takes time. Nevertheless, start simple and iterate. Consequently, you'll build reliable systems. Moreover, your team will thank you. Indeed, {keyphrase} done right transforms data engineering. Finally, go build something amazing!</p>"""

        return opening + why_section + architecture + implementation + best_practices + common_mistakes + closing
    
    def _generate_image_prompts(self, topic: Dict, content: str) -> List[Dict]:
        """Generate image prompts"""
        try:
            prompt = f"""Generate 4 image prompts for: {topic['title']}

Return JSON array:
[
  {{
    "placement": "hero",
    "prompt": "Hand-drawn technical sketch...",
    "alt_text": "SEO alt text",
    "caption": "Brief caption"
  }}
]"""
            return self._safe_generate(prompt, parse_json=True)
        except:
            return [{
                "placement": "hero",
                "prompt": f"Hand-drawn technical sketch of {topic['title']}",
                "alt_text": f"{topic['title']} overview",
                "caption": f"Overview of {topic['title']}"
            }]
    
    def _generate_references(self, topic: Dict) -> List[Dict]:
        """Generate reference links"""
        try:
            prompt = f"""Generate 5 reference links for: {topic['title']}

Return JSON array:
[
  {{
    "title": "Link title",
    "url": "https://...",
    "description": "Description"
  }}
]"""
            return self._safe_generate(prompt, parse_json=True)
        except:
            return [{
                "title": "Official Documentation",
                "url": "https://docs.snowflake.com",
                "description": "Official documentation and guides"
            }]
    
    def _calculate_reading_time(self, content: str) -> int:
        """Calculate reading time"""
        text = re.sub(r'<[^>]+>', '', content)
        words = len(text.split())
        return max(1, round(words / 225))


def main():
    """Test the enhanced generator"""
    from dotenv import load_dotenv
    load_dotenv()
    
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("‚ùå GEMINI_API_KEY not found in .env!")
        exit(1)
    
    generator = BlogGeneratorFree(api_key)
    
    test_topic = {
        'title': 'Snowflake Data Pipeline Complete Setup Guide',
        'category': 'snowflake',
        'keywords': ['snowflake', 'data pipeline', 'ETL'],
        'level': 'intermediate'
    }
    
    print("\nüöÄ Testing Enhanced Generator...\n")
    result = generator.generate_blog_post(test_topic)
    
    print("\n" + "="*60)
    print("ENHANCED BLOG POST GENERATED")
    print("="*60)
    print(f"\nüìù Title: {result['title']}")
    print(f"üéØ SEO Title: {result['seo']['title']}")
    print(f"üîë Focus Keyphrase: {result['seo']['focus_keyphrase']}")
    print(f"üîó Slug: {result['slug']}")
    print(f"üìä Word Count: {result['metadata']['word_count']}")
    print(f"‚úÖ Quality: {result['metadata']['quality']}")
    print(f"üí∞ Cost: {result['metadata']['cost']}")
    
    with open('test_blog_enhanced.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ Saved to: test_blog_enhanced.json")


if __name__ == "__main__":
    main()
