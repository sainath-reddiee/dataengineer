[
  {
    "id": "snowflake",
    "term": "Snowflake",
    "slug": "snowflake",
    "category": "data-warehousing",
    "shortDefinition": "A cloud-native data warehouse platform that separates storage and compute, enabling elastic scaling and pay-per-use pricing.",
    "fullDefinition": "Snowflake is a cloud-native data warehouse platform built for the modern data stack. Unlike traditional data warehouses, Snowflake was designed from the ground up for the cloud, offering unique architecture that separates storage and compute resources.\n\n## Key Architecture Features\n\nSnowflake uses a **multi-cluster shared data architecture** that consists of three layers:\n\n1. **Database Storage Layer**: Data is stored in a compressed, columnar format in cloud object storage (AWS S3, Azure Blob, or Google Cloud Storage). This layer is fully managed and automatically optimized.\n\n2. **Query Processing Layer (Virtual Warehouses)**: Compute clusters that execute queries independent of storage. You can spin up multiple warehouses of different sizes without affecting each other.\n\n3. **Cloud Services Layer**: Handles authentication, infrastructure management, metadata, query optimization, and access control.\n\n## Why Data Engineers Choose Snowflake\n\n- **Zero Management**: No indexes to tune, no partitions to manage\n- **Instant Elasticity**: Scale compute up/down in seconds\n- **Concurrency**: Multiple workloads without resource contention\n- **Time Travel**: Query historical data up to 90 days back\n- **Data Sharing**: Share live data across organizations securely\n- **Semi-structured Data**: Native support for JSON, Avro, Parquet\n\n## Snowflake vs Traditional Data Warehouses\n\nTraditional on-premise solutions like Teradata or Oracle require significant hardware investment and maintenance. Snowflake eliminates this with its SaaS model, offering true pay-per-second pricing and automatic performance optimization.\n\n## Common Use Cases\n\n- **Data Lakes**: Combine structured and semi-structured data\n- **Data Engineering**: Build scalable ETL/ELT pipelines\n- **Data Science**: Run ML workloads with Snowpark\n- **Business Intelligence**: Power dashboards with fast queries",
    "keyPoints": [
      "Cloud-native architecture separating storage and compute",
      "Pay-per-second pricing model",
      "Zero-maintenance with automatic optimization",
      "Time Travel feature for historical data access",
      "Native support for semi-structured data (JSON, Parquet)"
    ],
    "faqs": [
      {
        "question": "What is Snowflake used for?",
        "answer": "Snowflake is primarily used as a cloud data warehouse for storing, processing, and analyzing large volumes of structured and semi-structured data. It supports data engineering, analytics, data science, and data sharing use cases."
      },
      {
        "question": "Is Snowflake a database or data warehouse?",
        "answer": "Snowflake is a cloud data warehouse, not a traditional transactional database. It is optimized for analytical workloads (OLAP) rather than transactional operations (OLTP). However, it can store and query data like a database."
      },
      {
        "question": "How does Snowflake pricing work?",
        "answer": "Snowflake uses a consumption-based pricing model. You pay separately for storage (per TB/month) and compute (per credit consumed). Compute is charged per-second with a 60-second minimum, so you only pay when queries are running."
      },
      {
        "question": "What is Snowflake Time Travel?",
        "answer": "Time Travel is a Snowflake feature that lets you access historical data at any point within a defined retention period (up to 90 days). You can query, clone, or restore data as it existed at a specific timestamp."
      }
    ],
    "relatedTerms": [
      "data-warehouse",
      "snowpark",
      "dbt",
      "etl",
      "data-lake"
    ],
    "relatedTools": [
      "dbt",
      "Fivetran",
      "Airbyte",
      "Tableau",
      "Power BI"
    ],
    "externalLinks": [
      {
        "title": "Snowflake Official Documentation",
        "url": "https://docs.snowflake.com/"
      },
      {
        "title": "Snowflake Architecture Overview",
        "url": "https://docs.snowflake.com/en/user-guide/intro-key-concepts"
      }
    ],
    "keywords": [
      "snowflake",
      "cloud data warehouse",
      "snowflake data platform",
      "snowflake architecture",
      "snowflake pricing"
    ],
    "lastUpdated": "2026-01-21"
  },
  {
    "id": "data-warehouse",
    "term": "Data Warehouse",
    "slug": "data-warehouse",
    "category": "data-warehousing",
    "shortDefinition": "A centralized repository designed to store, integrate, and analyze large volumes of structured data from multiple sources for business intelligence and reporting.",
    "fullDefinition": "A data warehouse is a centralized repository that stores integrated data from multiple sources, optimized for analytical queries and reporting. Unlike operational databases designed for transactions (OLTP), data warehouses are built for analysis (OLAP).\n\n## Key Characteristics\n\n1. **Subject-Oriented**: Organized around business subjects (customers, products, sales) rather than applications\n\n2. **Integrated**: Data from disparate sources is cleansed, transformed, and unified into a consistent format\n\n3. **Time-Variant**: Historical data is preserved, enabling trend analysis over time\n\n4. **Non-Volatile**: Once data enters the warehouse, it's stable and doesn't change (unlike operational systems)\n\n## Data Warehouse Architecture\n\nModern data warehouses typically follow a layered architecture:\n\n- **Raw/Staging Layer**: Stores data as-is from source systems\n- **Integration Layer**: Cleaned and transformed data\n- **Presentation Layer**: Business-ready datasets for reporting and analytics\n- **Semantic Layer**: Business definitions and metrics\n\n## Cloud Data Warehouses\n\nThe industry has shifted from on-premise solutions to cloud-native platforms:\n\n| Platform | Provider | Key Feature |\n|----------|----------|-------------|\n| Snowflake | Independent | Separate storage/compute |\n| BigQuery | Google Cloud | Serverless, pay-per-query |\n| Redshift | AWS | Tight AWS integration |\n| Synapse | Azure | Unified analytics |\n| Databricks | Independent | Lakehouse architecture |\n\n## Data Warehouse vs Data Lake\n\n- **Data Warehouse**: Structured data, schema-on-write, optimized for BI\n- **Data Lake**: All data types, schema-on-read, optimized for data science\n- **Data Lakehouse**: Combines benefits of both (e.g., Databricks, Snowflake)\n\n## Benefits for Organizations\n\n- **Single Source of Truth**: Unified view across business domains\n- **Historical Analysis**: Track trends and patterns over time\n- **Performance**: Optimized for complex analytical queries\n- **Governance**: Centralized security and access control",
    "keyPoints": [
      "Centralized repository for analytical data",
      "Optimized for OLAP (analytical) workloads",
      "Stores historical data for trend analysis",
      "Integrates data from multiple source systems",
      "Cloud options include Snowflake, BigQuery, Redshift"
    ],
    "faqs": [
      {
        "question": "What is the purpose of a data warehouse?",
        "answer": "A data warehouse serves as a central repository for integrated data from multiple sources, enabling organizations to run complex analytical queries, generate reports, and make data-driven decisions."
      },
      {
        "question": "What is the difference between a database and a data warehouse?",
        "answer": "Databases (OLTP) are optimized for transactional operations like inserts and updates. Data warehouses (OLAP) are optimized for analytical queries across large datasets. Warehouses store historical data; databases typically store current state."
      },
      {
        "question": "What is ETL in data warehousing?",
        "answer": "ETL stands for Extract, Transform, Load‚Äîthe process of pulling data from source systems, transforming it into a consistent format, and loading it into the data warehouse for analysis."
      },
      {
        "question": "Is Snowflake a data warehouse?",
        "answer": "Yes, Snowflake is a cloud-native data warehouse platform. It provides all traditional data warehouse capabilities with modern features like separation of storage and compute, and native support for semi-structured data."
      }
    ],
    "relatedTerms": [
      "snowflake",
      "etl",
      "data-lake",
      "olap",
      "data-modeling"
    ],
    "relatedTools": [
      "Snowflake",
      "BigQuery",
      "Redshift",
      "Azure Synapse",
      "Databricks"
    ],
    "externalLinks": [
      {
        "title": "What is a Data Warehouse? - AWS",
        "url": "https://aws.amazon.com/data-warehouse/"
      },
      {
        "title": "Data Warehouse Concepts - Google Cloud",
        "url": "https://cloud.google.com/learn/what-is-a-data-warehouse"
      }
    ],
    "keywords": [
      "data warehouse",
      "data warehousing",
      "cloud data warehouse",
      "data warehouse vs data lake"
    ],
    "lastUpdated": "2026-01-21"
  },
  {
    "id": "data-lake",
    "term": "Data Lake",
    "slug": "data-lake",
    "category": "data-warehousing",
    "shortDefinition": "A centralized storage repository that holds vast amounts of raw data in its native format until needed for analysis, supporting structured, semi-structured, and unstructured data.",
    "fullDefinition": "A data lake is a centralized repository designed to store, process, and secure large volumes of data in any format‚Äîstructured, semi-structured, or unstructured. Unlike data warehouses that require data to be structured before storage, data lakes accept raw data as-is.\n\n## Data Lake vs Data Warehouse\n\n| Aspect | Data Lake | Data Warehouse |\n|--------|-----------|----------------|\n| Data Format | Raw, any format | Structured only |\n| Schema | Schema-on-read | Schema-on-write |\n| Users | Data scientists, engineers | Analysts, business users |\n| Processing | Batch and streaming | Primarily batch |\n| Cost | Lower storage cost | Higher, optimized storage |\n| Query Performance | Variable | Optimized for BI |\n\n## Data Lake Architecture\n\n### Zones\nModern data lakes organize data into zones:\n\n1. **Raw/Bronze Zone**: Data exactly as received from sources\n2. **Cleansed/Silver Zone**: Validated, deduplicated, standardized data\n3. **Curated/Gold Zone**: Business-ready, aggregated datasets\n\n### Data Lakehouse\nA new architecture combining data lake flexibility with warehouse performance:\n- **Delta Lake** (Databricks): ACID transactions on data lakes\n- **Apache Iceberg**: Open table format for huge datasets\n- **Apache Hudi**: Incremental data processing\n\n## Cloud Data Lake Platforms\n\n- **AWS**: S3 + Glue + Athena + EMR\n- **Azure**: Data Lake Storage + Synapse + Databricks\n- **Google Cloud**: GCS + Dataproc + BigQuery\n\n## Common Use Cases\n\n1. **Machine Learning**: Store training data in any format\n2. **Data Archival**: Cost-effective long-term storage\n3. **Data Exploration**: Analyze raw data before structuring\n4. **IoT Data**: Ingest high-volume sensor data\n5. **Log Analytics**: Store and analyze application logs\n\n## Challenges and Solutions\n\n- **Data Swamp**: Without governance, lakes become unusable ‚Üí Use data catalogs\n- **Query Performance**: Raw files are slow ‚Üí Use table formats (Delta, Iceberg)\n- **Security**: Sensitive data exposure ‚Üí Implement row/column level security",
    "keyPoints": [
      "Stores raw data in any format (structured, unstructured)",
      "Schema-on-read approach (define structure at query time)",
      "Lower cost than data warehouses for storage",
      "Lakehouse architecture combines lake + warehouse benefits",
      "Key platforms: S3, Azure Data Lake, Google Cloud Storage"
    ],
    "faqs": [
      {
        "question": "What is a data lake in simple terms?",
        "answer": "A data lake is a large storage system that holds raw data in its original format until you need to analyze it. Think of it as a \"dump\" for all your data‚Äîstructured or unstructured‚Äîthat can be processed later."
      },
      {
        "question": "What is the difference between data lake and data warehouse?",
        "answer": "Data warehouses store structured, processed data ready for BI. Data lakes store raw data in any format for flexible analysis. Warehouses are faster for queries; lakes are cheaper for storage."
      },
      {
        "question": "What is a data lakehouse?",
        "answer": "A data lakehouse combines the low-cost, flexible storage of a data lake with the performance and ACID transactions of a data warehouse. Technologies like Delta Lake, Iceberg, and Hudi enable this architecture."
      },
      {
        "question": "What are the benefits of a data lake?",
        "answer": "Benefits include: storing any data type, lower storage costs, flexibility for data science, scalability, and the ability to keep raw data for future use cases you have not yet defined."
      }
    ],
    "relatedTerms": [
      "data-warehouse",
      "snowflake",
      "databricks",
      "delta-lake",
      "s3"
    ],
    "relatedTools": [
      "AWS S3",
      "Azure Data Lake",
      "Databricks",
      "Delta Lake",
      "Apache Iceberg"
    ],
    "externalLinks": [
      {
        "title": "What is a Data Lake? - AWS",
        "url": "https://aws.amazon.com/big-data/datalakes-and-analytics/what-is-a-data-lake/"
      },
      {
        "title": "Data Lake vs Lakehouse - Databricks",
        "url": "https://www.databricks.com/glossary/data-lakehouse"
      }
    ],
    "keywords": [
      "data lake",
      "data lakehouse",
      "data lake vs data warehouse",
      "delta lake",
      "data lake architecture"
    ],
    "lastUpdated": "2026-01-21"
  },
  {
    "id": "scd",
    "term": "Slowly Changing Dimensions (SCD)",
    "slug": "scd",
    "category": "data-warehousing",
    "shortDefinition": "A concept in data warehousing to manage how data that changes slowly over time is stored and tracked.",
    "fullDefinition": "SCDs are used to track historical data in dimension tables.\n\n## Common Types\n- **Type 0**: Fixed (No changes allowed)\n- **Type 1**: Overwrite (No history)\n- **Type 2**: Add new row (Full history with validity dates)\n- **Type 3**: Add new column (Limited history)",
    "keyPoints": [
      "History Tracking",
      "Dimensional Modeling",
      "Data Warehousing"
    ],
    "relatedTerms": [
      "data-warehouse",
      "data-modeling",
      "star-schema"
    ],
    "relatedTools": [
      "dbt",
      "Informatica"
    ],
    "externalLinks": [
      {
        "title": "SCD Types",
        "url": "https://en.wikipedia.org/wiki/Slowly_changing_dimension"
      }
    ],
    "keywords": [
      "scd",
      "slowly changing dimensions",
      "type 2 dimension"
    ],
    "lastUpdated": "2026-01-21"
  },
  {
    "id": "columnar-storage",
    "term": "Columnar Storage",
    "slug": "columnar-storage",
    "category": "data-warehousing",
    "shortDefinition": "A database management system that stores data in columns rather than rows, optimized for analytics.",
    "fullDefinition": "Columnar storage saves data by column rather than by row. This is highly efficient for analytical queries (OLAP) which typically aggregate a few columns over many rows.\n\n## Benefits\n- **Compression**: Similar data types in columns compress very well (10x-50x).\n- **IO Efficiency**: Only read usage columns, ignore the rest.",
    "keyPoints": [
      "OLAP optimization",
      "Compression",
      "Analytics"
    ],
    "relatedTerms": [
      "olap",
      "data-warehouse",
      "row-oriented"
    ],
    "relatedTools": [
      "Snowflake",
      "Redshift",
      "BigQuery",
      "Parquet"
    ],
    "externalLinks": [
      {
        "title": "Columnar Database",
        "url": "https://en.wikipedia.org/wiki/Column-oriented_DBMS"
      }
    ],
    "keywords": [
      "columnar",
      "olap",
      "compression",
      "parquet"
    ],
    "lastUpdated": "2026-01-21"
  },
  {
    "term": "Lakehouse Architecture",
    "slug": "lakehouse",
    "category": "data-warehousing",
    "shortDefinition": "A modern data architecture that combines the performance and governance of data warehouses with the low-cost and flexibility of data lakes.",
    "fullDefinition": "The Lakehouse architecture is an attempt to merge the best of both worlds: the structure and ACID compliance of a Data Warehouse with the scalability and cost-efficiency of a Data Lake. It usually sits on top of open table formats like Delta Lake or Apache Iceberg.",
    "keyPoints": [
      "ACID transactions on object storage (S3/GCS)",
      "Supports both BI (SQL) and AI/ML (Python/Spark)",
      "Eliminates data silos by having one source of truth",
      "Low-cost storage with high-performance query engines"
    ]
  },
  {
    "term": "Table Format",
    "slug": "table-format",
    "category": "data-warehousing",
    "shortDefinition": "A layer that organizes files in a data lake into a structured table, enabling SQL-like features like ACID and time-travel.",
    "fullDefinition": "Table formats like Iceberg, Hudi, and Delta Lake sit on top of Parquet files to provide metadata. This metadata allows query engines to know which files belong to a table, enabling ACID transactions and hidden partitioning.",
    "keyPoints": [
      "Enables ACID compliance on S3/GCS",
      "Supports Schema Evolution and Time Travel",
      "Engine agnostic (usually)",
      "Replaces the legacy Hive Metastore approach"
    ]
  },
  {
    "term": "Medallion Architecture",
    "slug": "medallion-architecture",
    "category": "data-warehousing",
    "shortDefinition": "A data design pattern that organizes data into three layers ‚Äî Bronze (raw), Silver (cleaned), and Gold (business-ready) ‚Äî to progressively improve data quality in a lakehouse.",
    "fullDefinition": "The **Medallion Architecture** (also called Multi-Hop Architecture) is the most popular design pattern for organizing data in a **Data Lakehouse**. Popularized by Databricks, it divides your data pipeline into three distinct quality layers:\n\n## The Three Layers\n\n### ü•â Bronze Layer (Raw)\nThe landing zone for raw, unprocessed data:\n- **What goes here**: Exact copies of source data ‚Äî JSON payloads, CDC logs, CSV dumps\n- **Format**: Usually stored as-is, with added metadata (ingestion timestamp, source system)\n- **Purpose**: Single source of truth; you can always replay from bronze\n- **Example**: Raw Salesforce API responses, Kafka event streams, database replication logs\n\n### ü•à Silver Layer (Cleansed)\nData that has been validated, deduplicated, and conformedData:\n- **What happens here**: Schema enforcement, null handling, deduplication, type casting\n- **Format**: Strongly typed, partitioned, stored in columnar format (Parquet/Delta)\n- **Purpose**: Enterprise-wide \"clean\" data that is usable across teams\n- **Example**: A unified `customers` table combining Salesforce + Stripe + internal DB records\n\n### ü•á Gold Layer (Business-Ready)\nAggregated, enriched data tailored for specific business use cases:\n- **What goes here**: Business metrics, KPI tables, feature stores, ML training sets\n- **Format**: Star/snowflake schemas optimized for BI tools\n- **Purpose**: Powers dashboards, reports, and ML models directly\n- **Example**: `daily_revenue_by_region`, `customer_lifetime_value`, `churn_predictions`\n\n## Why It Works\n\n```\nSource Systems ‚Üí [Bronze] ‚Üí [Silver] ‚Üí [Gold] ‚Üí BI / ML\n                   Raw        Clean      Aggregated\n                   Append     Validated   Business Logic\n                   Replay     Conformed   Star Schema\n```\n\n## Key Benefits\n\n- **Incremental Processing**: Each layer only processes what changed\n- **Debuggability**: When something breaks, trace it back through the layers\n- **Reusability**: Silver layer serves multiple Gold-layer consumers\n- **Governance**: Apply access controls at the appropriate layer\n\n## Medallion Architecture Tools\n\n| Layer | Common Tools |\n|-------|-------------|\n| Bronze | Fivetran, Airbyte, Kafka Connect, AWS Glue |\n| Silver | dbt, Spark, Snowflake Tasks, Dataform |\n| Gold | dbt, Looker, Power BI, Tableau |\n\n## Anti-Patterns to Avoid\n\n1. **Skipping Silver**: Going directly from Bronze to Gold creates fragile pipelines\n2. **Too Many Layers**: Some teams add Platinum, Diamond ‚Äî keep it simple\n3. **No Schema Enforcement**: Silver should enforce schemas strictly\n4. **Ignoring Bronze Retention**: Bronze is your backup; don't delete it too aggressively",
    "keyPoints": [
      "Three-layer data quality pattern: Bronze ‚Üí Silver ‚Üí Gold",
      "Bronze stores raw data as-is for replayability",
      "Silver cleanses, deduplicates, and conforms data",
      "Gold provides business-ready aggregated datasets",
      "Popularized by Databricks for lakehouse architectures"
    ],
    "faqs": [
      {
        "question": "What is the Medallion Architecture?",
        "answer": "The Medallion Architecture is a data design pattern that organizes data into three progressive quality layers ‚Äî Bronze (raw), Silver (cleansed), and Gold (business-ready) ‚Äî in a lakehouse environment."
      },
      {
        "question": "What is the difference between Bronze Silver and Gold layers?",
        "answer": "Bronze stores raw unprocessed data exactly as ingested. Silver cleans, validates, and deduplicates that data. Gold applies business logic to create aggregated, analytics-ready datasets."
      },
      {
        "question": "Is Medallion Architecture only for Databricks?",
        "answer": "No. While Databricks popularized it, the Medallion Architecture works on any platform ‚Äî Snowflake, BigQuery, Azure Synapse, or open-source tools like dbt + Apache Iceberg."
      },
      {
        "question": "Can I use Medallion Architecture with Snowflake?",
        "answer": "Yes. You can implement it using Snowflake databases or schemas for each layer (e.g., RAW_DB, CLEAN_DB, ANALYTICS_DB) and use dbt or Snowflake Tasks for transformations."
      }
    ],
    "relatedTerms": [
      "data-warehouse",
      "lakehouse",
      "etl",
      "data-modeling",
      "dbt"
    ],
    "relatedTools": [
      "Databricks",
      "dbt",
      "Snowflake",
      "Apache Iceberg",
      "Delta Lake"
    ],
    "externalLinks": [
      {
        "title": "Databricks Medallion Architecture Guide",
        "url": "https://www.databricks.com/glossary/medallion-architecture"
      },
      {
        "title": "Implementing Medallion with dbt",
        "url": "https://docs.getdbt.com/best-practices"
      }
    ],
    "keywords": [
      "medallion architecture",
      "bronze silver gold data",
      "medallion architecture databricks",
      "data lakehouse layers",
      "multi-hop architecture"
    ],
    "lastUpdated": "2026-02-27"
  },
  {
    "term": "Apache Iceberg",
    "slug": "apache-iceberg",
    "category": "data-warehousing",
    "shortDefinition": "An open table format for huge analytic datasets that brings warehouse-like features (ACID transactions, time travel, schema evolution) to data lakes.",
    "fullDefinition": "**Apache Iceberg** is an open table format designed for petabyte-scale analytic datasets. Created at Netflix, it solves the fundamental problems that made data lakes unreliable ‚Äî by bringing **ACID transactions**, **time travel**, **schema evolution**, and **partition evolution** to files sitting in object storage (S3, GCS, ADLS).\n\n## The Problem Iceberg Solves\n\nTraditional data lakes stored data as raw files (Parquet/ORC) with a Hive Metastore tracking partitions. This architecture had critical flaws:\n\n- **No ACID**: Concurrent writes could corrupt data\n- **Partition Lock-in**: Changing partition schemes required rewriting all data\n- **No Time Travel**: Can't query historical states\n- **Schema Rigidity**: Adding columns was painful\n- **Small File Problem**: Too many small files destroyed performance\n\nIceberg fixes ALL of these.\n\n## How Iceberg Works\n\n```\nQuery Engine (Spark/Trino/Flink)\n        ‚îÇ\n        ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Iceberg Catalog ‚îÇ (Where is the latest table state?)\n‚îÇ  (REST/Glue/Nessie)‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Metadata Layer  ‚îÇ (Manifest files + Manifest Lists)\n‚îÇ  - Snapshots    ‚îÇ\n‚îÇ  - Manifests    ‚îÇ\n‚îÇ  - Statistics   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Data Files     ‚îÇ (Parquet/ORC/Avro in S3/GCS/ADLS)\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Key Features\n\n### ACID Transactions\n- Atomic commits: writes either fully succeed or fully fail\n- Snapshot isolation for concurrent reads and writes\n- No more corrupted tables from failed jobs\n\n### Time Travel\n```sql\n-- Query data as it was 24 hours ago\nSELECT * FROM orders\nFOR SYSTEM_TIME AS OF TIMESTAMP '2026-02-26 09:00:00';\n\n-- Roll back to a previous snapshot\nALTER TABLE orders ROLLBACK TO SNAPSHOT 12345;\n```\n\n### Schema Evolution\n- Add, drop, rename, or reorder columns without rewriting data\n- Type promotion (int ‚Üí long, float ‚Üí double) is safe\n- No downtime for schema changes\n\n### Partition Evolution\n- Change partition schemes without rewriting data\n- Start with daily partitions, switch to hourly ‚Äî Iceberg handles it\n- Hidden partitioning: users don't need to know partition columns\n\n### Engine Compatibility\nIceberg tables can be read/written by multiple engines simultaneously:\n- Apache Spark, Trino/Presto, Flink, Snowflake, BigQuery, Dremio, StarRocks\n\n## Iceberg vs Delta Lake vs Hudi\n\n| Feature | Iceberg | Delta Lake | Hudi |\n|---------|---------|------------|------|\n| Governance | Apache Foundation | Databricks | Apache Foundation |\n| Engine Lock-in | None | Spark-first | Spark-first |\n| Partition Evolution | ‚úÖ Best-in-class | ‚ùå | Partial |\n| Catalog Options | REST, Glue, Nessie, Polaris | Unity Catalog | Hive Metastore |\n| Adoption Trend | üìà Fastest growing | üìà Strong (Databricks) | ‚û°Ô∏è Stable |",
    "keyPoints": [
      "Open table format for petabyte-scale analytics on data lakes",
      "ACID transactions, time travel, schema evolution, partition evolution",
      "Engine-agnostic: works with Spark, Trino, Flink, Snowflake, BigQuery",
      "Solves the reliability problems of traditional Hive-based data lakes",
      "Created at Netflix, now an Apache Foundation project"
    ],
    "faqs": [
      {
        "question": "What is Apache Iceberg used for?",
        "answer": "Apache Iceberg is used to manage large analytical datasets in data lakes. It provides warehouse-like features (ACID transactions, time travel, schema evolution) to files stored in S3, GCS, or ADLS."
      },
      {
        "question": "Is Apache Iceberg better than Delta Lake?",
        "answer": "Iceberg is more engine-agnostic and has superior partition evolution. Delta Lake is tightly integrated with Databricks and Spark. The best choice depends on your stack ‚Äî Iceberg for multi-engine environments, Delta for Databricks shops."
      },
      {
        "question": "Does Snowflake support Apache Iceberg?",
        "answer": "Yes. Snowflake supports Iceberg Tables natively, allowing you to query and manage Iceberg-formatted data in S3/GCS/ADLS directly from Snowflake. This is a core part of Snowflake's open data strategy."
      },
      {
        "question": "What is the difference between Iceberg and Parquet?",
        "answer": "Parquet is a file format (how data is stored). Iceberg is a table format (how files are organized and managed). Iceberg typically uses Parquet files underneath but adds metadata, transactions, and versioning on top."
      }
    ],
    "relatedTerms": [
      "table-format",
      "lakehouse",
      "data-lake",
      "columnar-storage",
      "data-warehouse"
    ],
    "relatedTools": [
      "Snowflake",
      "Databricks",
      "Trino",
      "Apache Spark",
      "Nessie"
    ],
    "externalLinks": [
      {
        "title": "Apache Iceberg Documentation",
        "url": "https://iceberg.apache.org/docs/latest/"
      },
      {
        "title": "Iceberg Spec",
        "url": "https://iceberg.apache.org/spec/"
      }
    ],
    "keywords": [
      "apache iceberg",
      "iceberg table format",
      "iceberg vs delta lake",
      "iceberg snowflake",
      "what is apache iceberg"
    ],
    "lastUpdated": "2026-02-27"
  }
]
